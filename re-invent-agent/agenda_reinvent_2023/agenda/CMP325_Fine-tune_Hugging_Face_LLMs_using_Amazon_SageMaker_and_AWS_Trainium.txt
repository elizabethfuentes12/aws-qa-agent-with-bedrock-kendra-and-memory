location: Mandalay Bay, Level 2 | South, Surf D
summary: tag_name :AI/ML, Topic, Compute, Topic, Cost Optimization, Area of Interest, Generative AI, Area of Interest, Well-Architected Framework, Area of Interest, Cross Industry, Industry, 300 - Advanced, Level, Data Scientist, Role, Developer/Engineer, Role, Solution/Systems Architect, Role, Amazon Elastic Compute Cloud (EC2), Services, Amazon SageMaker, Services, Monday, Day, Mandalay Bay, Venue, , speakers :firstName_0: Jianying, lastName_0: Lang, firstName_1: Matthew, lastName_1: McClean, firstName_2: KC, lastName_2: Tung, firstName_3: Scott, lastName_3: Perry, firstName_4: Max, lastName_4: Liu, , thirdpartyid :CMP325, sessionuid :85571A53-D5E6-46A4-BB25-3161120D92C7, title :Fine-tune Hugging Face LLMs using Amazon SageMaker and AWS Trainium, description :<p>Large language models (LLMs) are pretrained on vast amounts of data and perform well across a variety of general-purpose tasks and benchmarks without further specialized training. In practice, however, it is common to improve the performance of a pretrained LLM by fine-tuning the model using a smaller, task-specific or domain-specific dataset. In this builders’ session, learn how to use Amazon SageMaker to fine-tune a pretrained Hugging Face LLM using AWS Trainium accelerators, and then leverage the fine-tuned model for inference. You must bring your laptop to participate.</p>, sessiontype :standard, venuename :Mandalay Bay, floorplanname :Level 2 | South, locationname :Surf D, startdatetimeutc :November, 27 2023 16:30:00 -0500, enddatetimeutc :November, 27 2023 17:30:00 -0500, 
enddatetimeutc: November, 27 2023 17:30:00 -0500
startdatetimeutc: November, 27 2023 16:30:00 -0500
locationname: Surf D
floorplanname: Level 2 | South
venuename: Mandalay Bay
sessiontype: standard
description: <p>Large language models (LLMs) are pretrained on vast amounts of data and perform well across a variety of general-purpose tasks and benchmarks without further specialized training. In practice, however, it is common to improve the performance of a pretrained LLM by fine-tuning the model using a smaller, task-specific or domain-specific dataset. In this builders’ session, learn how to use Amazon SageMaker to fine-tune a pretrained Hugging Face LLM using AWS Trainium accelerators, and then leverage the fine-tuned model for inference. You must bring your laptop to participate.</p>
title: Fine-tune Hugging Face LLMs using Amazon SageMaker and AWS Trainium
sessionuid: 85571A53-D5E6-46A4-BB25-3161120D92C7
thirdpartyid: CMP325
speakers: firstName_0: Jianying, lastName_0: Lang, firstName_1: Matthew, lastName_1: McClean, firstName_2: KC, lastName_2: Tung, firstName_3: Scott, lastName_3: Perry, firstName_4: Max, lastName_4: Liu
tag_name: AI/ML, Topic, Compute, Topic, Cost Optimization, Area of Interest, Generative AI, Area of Interest, Well-Architected Framework, Area of Interest, Cross Industry, Industry, 300 - Advanced, Level, Data Scientist, Role, Developer/Engineer, Role, Solution/Systems Architect, Role, Amazon Elastic Compute Cloud (EC2), Services, Amazon SageMaker, Services, Monday, Day, Mandalay Bay, Venue
