location: Venetian, Level 2, Hall B, Expo, Theater 5, Booth #1130, Data Zone
summary: tag_name :AI/ML, Topic, Databases, Topic, Resilience, Area of Interest, Cost Optimization, Area of Interest, Cross Industry, Industry, 100 - Foundational, Level, Developer/Engineer, Role, DevOps Engineer, Role, Solution/Systems Architect, Role, Wednesday, Day, Venetian, Venue, , speakers :firstName_0: Sam, lastName_0: Partee, company_0: Redis, , thirdpartyid :DAT101-S, sessionuid :CCBAF7B0-9AFA-45E9-86F1-9AF1062DA549, title :Real-time RAG: How to augment LLMs with Redis and Amazon Bedrock (sponsored by Redis), description :<p>Large language models (LLMs), such as GPT-4, use the power of vector embeddings and databases to address challenges posed by evolving data. These embeddings, when combined with a vector database or search algorithm, offer a way for LLMs to gain access to an up-to-date and ever-expanding knowledge base. This ensures LLMs remain capable of generating accurate and contextually appropriate outputs, even in the face of constantly changing information. This approach is sometimes called Retrieval Augmented Generation (RAG). In this lightning talk, learn about RAG and the benefits of using Redis Enterprise as a vector database with Amazon Bedrock. This presentation is brought to you by Redis, an AWS Partner.</p>, sessiontype :standard, venuename :Venetian, floorplanname :Level 2, locationname :Hall B, Expo, Theater 5, Booth #1130, Data Zone, startdatetimeutc :November, 29 2023 20:00:00 -0500, enddatetimeutc :November, 29 2023 20:20:00 -0500, 
enddatetimeutc: November, 29 2023 20:20:00 -0500
startdatetimeutc: November, 29 2023 20:00:00 -0500
locationname: Hall B, Expo, Theater 5, Booth #1130, Data Zone
floorplanname: Level 2
venuename: Venetian
sessiontype: standard
description: <p>Large language models (LLMs), such as GPT-4, use the power of vector embeddings and databases to address challenges posed by evolving data. These embeddings, when combined with a vector database or search algorithm, offer a way for LLMs to gain access to an up-to-date and ever-expanding knowledge base. This ensures LLMs remain capable of generating accurate and contextually appropriate outputs, even in the face of constantly changing information. This approach is sometimes called Retrieval Augmented Generation (RAG). In this lightning talk, learn about RAG and the benefits of using Redis Enterprise as a vector database with Amazon Bedrock. This presentation is brought to you by Redis, an AWS Partner.</p>
title: Real-time RAG: How to augment LLMs with Redis and Amazon Bedrock (sponsored by Redis)
sessionuid: CCBAF7B0-9AFA-45E9-86F1-9AF1062DA549
thirdpartyid: DAT101-S
speakers: firstName_0: Sam, lastName_0: Partee, company_0: Redis
tag_name: AI/ML, Topic, Databases, Topic, Resilience, Area of Interest, Cost Optimization, Area of Interest, Cross Industry, Industry, 100 - Foundational, Level, Developer/Engineer, Role, DevOps Engineer, Role, Solution/Systems Architect, Role, Wednesday, Day, Venetian, Venue
