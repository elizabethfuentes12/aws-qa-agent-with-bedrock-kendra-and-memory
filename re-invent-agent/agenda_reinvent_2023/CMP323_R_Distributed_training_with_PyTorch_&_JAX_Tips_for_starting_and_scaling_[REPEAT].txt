location: Caesars Forum, Level 1, Alliance 305
summary: tag_name :Well-Architected Framework, Area of Interest, AI/ML, Topic, Compute, Topic, Cost Optimization, Area of Interest, 300 - Advanced, Level, Solution/Systems Architect, Role, Amazon Elastic Compute Cloud (EC2), Services, Monday, Day, Caesars Forum, Venue, Generative AI, Area of Interest, Data Scientist, Role, Amazon Elastic Kubernetes Service (EKS), Services, Cross Industry, Industry, AWS ParallelCluster, Services, Developer/Engineer, Role, , speakers :firstName_0: Keita, lastName_0: Watanabe, firstName_1: Uros, lastName_1: Lipovsek, , thirdpartyid :CMP323-R, sessionuid :58BB3DC9-1115-4D55-A4DB-CB403B659A03, title :Distributed training with PyTorch & JAX: Tips for starting and scaling [REPEAT], description :<p>Distributed training can be daunting for nonexperts because it requires a deep understanding of your architecture, software stack, and runtime configurations to be efficient. In addition, the scale required to train large foundation models amplifies the complexity to optimize and operate such systems, because any performance issue can have a dramatic effect on the utilization of your computational resources. In this session, learn the best practices, common mistakes, and reference architectures to build your models with PyTorch and JAX. Learn how to optimally choose Amazon EC2 instances and orchestration services for distributed training on thousands of devices.</p>, sessiontype :Chalk Talk, venuename :Caesars Forum, floorplanname :Level 1, locationname :Alliance 305, startdatetimeutc :November, 27 2023 19:30:00 -0500, enddatetimeutc :November, 27 2023 20:30:00 -0500, 
enddatetimeutc: November, 27 2023 20:30:00 -0500
startdatetimeutc: November, 27 2023 19:30:00 -0500
locationname: Alliance 305
floorplanname: Level 1
venuename: Caesars Forum
sessiontype: Chalk Talk
description: <p>Distributed training can be daunting for nonexperts because it requires a deep understanding of your architecture, software stack, and runtime configurations to be efficient. In addition, the scale required to train large foundation models amplifies the complexity to optimize and operate such systems, because any performance issue can have a dramatic effect on the utilization of your computational resources. In this session, learn the best practices, common mistakes, and reference architectures to build your models with PyTorch and JAX. Learn how to optimally choose Amazon EC2 instances and orchestration services for distributed training on thousands of devices.</p>
title: Distributed training with PyTorch & JAX: Tips for starting and scaling [REPEAT]
sessionuid: 58BB3DC9-1115-4D55-A4DB-CB403B659A03
thirdpartyid: CMP323-R
speakers: firstName_0: Keita, lastName_0: Watanabe, firstName_1: Uros, lastName_1: Lipovsek
tag_name: Well-Architected Framework, Area of Interest, AI/ML, Topic, Compute, Topic, Cost Optimization, Area of Interest, 300 - Advanced, Level, Solution/Systems Architect, Role, Amazon Elastic Compute Cloud (EC2), Services, Monday, Day, Caesars Forum, Venue, Generative AI, Area of Interest, Data Scientist, Role, Amazon Elastic Kubernetes Service (EKS), Services, Cross Industry, Industry, AWS ParallelCluster, Services, Developer/Engineer, Role
