{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import load_tools,Tool\n",
    "from langchain.memory.chat_message_histories import DynamoDBChatMessageHistory\n",
    "from langchain.agents import initialize_agent, AgentType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb_resource=boto3.resource('dynamodb')\n",
    "boto3_bedrock = boto3.client(\"bedrock-runtime\")\n",
    "kendra_client=boto3.client(service_name='kendra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb_resource=boto3.resource('dynamodb')\n",
    "boto3_bedrock = boto3.client(\"bedrock-runtime\")\n",
    "kendra_client=boto3.client(service_name='kendra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table_name_active_connections = os.environ.get('TABLE_CONNECTIONS')\n",
    "table_name = \"SessionTable\"\n",
    "table_name_agenda = \"QaToAnAgendaStack-TblReInventAgendaB6AA89B7-1O9IOB868HBRH\"\n",
    "kendra_index_id = \"04a17bad-8a17-4893-8f62-75341e5e789e\"\n",
    "lambda_query_function_name = \"QaToAnAgendaStack-FndynamodbqueryC2D26368-RxuymCNMoCBm\"\n",
    "bedrock_embedding_model_id = 'amazon.titan-embed-text-v1'\n",
    "bedrock_model_id = \"anthropic.claude-instant-v1\"\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = os.environ.get('TABLE_SESSION')\n",
    "table_name_agenda = os.environ.get('TABLE_NAME')\n",
    "kendra_index_id = os.environ.get('KENDRA_INDEX')\n",
    "lambda_query_function_name = os.environ.get('LAMBDA_QUERY_NAME')\n",
    "bedrock_model_id = os.environ.get('MODEL_ID')\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Bedrock(model_id=bedrock_model_id, model_kwargs=model_parameter,client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendra_retriever(kendra_index_id,llm):\n",
    "    retriever = AmazonKendraRetriever(index_id=kendra_index_id)\n",
    "    memory_kendra = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True,ai_prefix=\"A\",human_prefix=\"H\")\n",
    "\n",
    "\n",
    "    Kendra_prompt_template = \"\"\"Human: You are an Agenda re:Invent 2023 Assistant. \n",
    "    You are talkative and provides specific details from its context.\n",
    "    If the AI does not know the answer to a question, it truthfully says it \n",
    "    does not know.\n",
    "\n",
    "    Assistant: OK, got it, I'll be a talkative truthful AI assistant.\n",
    "\n",
    "    Human: Here are a few documents in <documents> tags:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Based on previous documents, provide a list of data for each documents (If you find more than one), example: id, speaker, veneu, date... , it's okay if you respond with just one option, the information is important so I can decide the best choise for me to: {question}\n",
    "\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=Kendra_prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    condense_qa_template_kendra = \"\"\"{chat_history}\n",
    "    Human:\n",
    "    Given the previous conversation and a follow up question below, based on previous documents, provide a list of data for each document (If you find more than one), example: id, speaker, veneu, date... , it's okay if you respond with just one option, the information is important so I can decide the best choise for me.\n",
    "    Follow Up Question: {question}\n",
    "    Standalone Question:\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "    standalone_question_prompt_kendra = PromptTemplate.from_template(condense_qa_template_kendra)\n",
    "\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, \n",
    "        retriever=retriever, \n",
    "        condense_question_prompt=standalone_question_prompt_kendra, \n",
    "        return_source_documents=False, \n",
    "        combine_docs_chain_kwargs={\"prompt\":PROMPT},\n",
    "        memory = memory_kendra,\n",
    "        #verbose=True\n",
    "        )\n",
    "\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_kendra_tools(qa,tools):\n",
    "    tools.append(\n",
    "    Tool.from_function(\n",
    "        func=qa.run,\n",
    "        name=\"re-invent-agenda-2023\",\n",
    "        description=\"useful when you want search sessions in re:Invent 2023 agenda. This will output documentation in text type, and /n, you must deliver a coherent and accurate response from all the documentation provided. \",\n",
    "    )\n",
    "    )\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promp_definition():\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "        You are an assistant who provides recommendations on re:invent 2023 sessions, you deliver the recommendation according to their question, and also do casual conversation. \n",
    "        Use the following format:\n",
    "        History: the context of a previous conversation with the user. Useful if you need to recall past conversation, make a summary, or rephrase the answers. if History is empty it continues.\n",
    "        Question: the input question you must answer\n",
    "        Thought: you should always think about what to do, Also try to follow steps mentioned above.\n",
    "        Action: the action to take, should be one of ['re-invent-agenda-2023',\"search-session-id-information\"], provides options that have the most information that can be useful in making a decision.\n",
    "        Action Input: the input to the action\n",
    "        Observation: the result of the action\n",
    "        Thought: I now know the final answer\n",
    "        Final Answer: the final answer to the original input question, If you have more than one answer, give them all. Always reply in the original user language and human legible.\n",
    "\n",
    "        History: \n",
    "        {chat_history}\n",
    "\n",
    "        Question: {input}\n",
    "\n",
    "        Assistant:\n",
    "        {agent_scratchpad}\"\"\"\n",
    "\n",
    "    updated_prompt = PromptTemplate(\n",
    "    input_variables=['chat_history','input', 'agent_scratchpad'], template=prompt_template)\n",
    "\n",
    "    return updated_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_dynamodb(id,table_name_session,llm):\n",
    "    message_history = DynamoDBChatMessageHistory(table_name=table_name_session, session_id=id)\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\", llm=llm,max_token_limit=800,chat_memory=message_history, return_messages=True,ai_prefix=\"A\",human_prefix=\"H\"\n",
    "    )\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_agent(memory,tools,llm):\n",
    "    zero_shot_agent=initialize_agent(\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    #verbose=True,\n",
    "    max_iteration=1,\n",
    "    #return_intermediate_steps=True,\n",
    "    handle_parsing_errors=True,\n",
    "    memory=memory\n",
    ")\n",
    "    return zero_shot_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_to_an_agenda (prompt,session_id):\n",
    "\n",
    "    tools = load_tools(\n",
    "                        [\"awslambda\"],\n",
    "                        awslambda_tool_name=\"search-session-id-information\",\n",
    "                        awslambda_tool_description=\"useful for searching session information data by their ID number, only send the ID number\",\n",
    "                        function_name=lambda_query_function_name,\n",
    "                    )\n",
    "\n",
    "    qa = kendra_retriever(kendra_index_id,llm)\n",
    "    tools = define_kendra_tools(qa,tools)\n",
    "    memory  = memory_dynamodb(session_id,table_name,llm)\n",
    "    agent = langchain_agent(memory,tools,llm)\n",
    "    agent.agent.llm_chain.prompt=promp_definition()\n",
    "    completion = agent(prompt)\n",
    "\n",
    "    print(completion['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I want know about the session with id BOA212\"\n",
    "session_id = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_to_an_agenda (prompt,session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Did you know a similar session?\"\n",
    "session_id = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_to_an_agenda (prompt,session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I want know about generative ai session\"\n",
    "session_id = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_to_an_agenda (prompt,session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
